# Real-time-sign-language-Translator-A-Machine-Learning-based-approach
This project is a real-time computer vision based application that captures live webcam input
and translates hand sign language gestures into readable text.

The system uses a CNN-based deep learning model trained on sign language data and performs
real-time gesture prediction using OpenCV. The project is designed to improve accessibility
for speech- and hearing-impaired individuals by enabling seamless humanâ€“computer interaction.

ğŸ”¹ Features
- Real-time webcam-based gesture recognition
- Converts hand gestures into text instantly
- CNN-based deep learning model
- Smooth and responsive UI for real-time usage
- Designed for accessibility and assistive technology use cases

ğŸ”¹ Technologies Used
- Python
- Machine Learning
- Convolutional Neural Networks (CNN)
- OpenCV
- NumPy
- Computer Vision

ğŸ”¹ Project Structure

sign_language_polished_UI.py   -> Main real-time prediction script with UI

test_model_output.py          -> Script to test trained model predictions

ğŸ”¹ Note

â€œDue to file size and ownership considerations, the trained model file (.h5) is not included. The complete architecture, preprocessing, and inference code are provided.â€

ğŸ”¹ How to Run

1. Clone the repository
2. Install required dependencies
3. Run sign_language_polished_UI.py
4. Allow webcam access

ğŸ”¹ Author

Aditi Gaure
